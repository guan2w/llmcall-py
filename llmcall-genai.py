#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
LLM æ‰¹é‡è°ƒç”¨ï¼šè¯»å– Excelï¼ˆprompt & QAï¼‰ï¼ŒæŒ‰ QA çš„ Q åˆ—é€è¡Œè¯·æ±‚ LLMï¼Œ
å°† JSON æ•°ç»„ç»“æœå±•å¼€å†™å›åŸæ–‡ä»¶ã€‚æ»¡è¶³ï¼š
- å±•å¼€ç»“æœçš„æ‰€æœ‰è¡Œï¼šQ ä¸ æ˜¯å¦æ‰¾åˆ° ç›¸åŒ
- æ¯ä¸ªè¾“å…¥å¤„ç†å®Œæˆåç«‹å³è½ç›˜
- æ”¯æŒ rows èŒƒå›´ã€æ–­ç‚¹ç»­è·‘ã€å¹¶å‘è¯·æ±‚ï¼ˆè¯·æ±‚å¹¶å‘ï¼Œå†™å…¥ä¸²è¡Œï¼‰

æœ¬ç‰ˆæœ¬ä½¿ç”¨ Google GenAI SDK (google-genai)
"""

import argparse
import datetime as dt
import json
import os
import re
import sys
import time
from typing import Any, Dict, List, Optional, Tuple

from google import genai
from google.genai import types
from openpyxl import load_workbook
from openpyxl.worksheet.worksheet import Worksheet

# --- åˆ—åé…ç½® ---
COL_FOUND = "FOUND"  # ç»“æœçŠ¶æ€åˆ—å
COL_ERROR = "ERROR"      # é”™è¯¯ä¿¡æ¯åˆ—å

# --- é…ç½®è§£æï¼ˆtomllib ä¼˜å…ˆï¼‰ ---
try:
    import tomllib  # py311+
except ModuleNotFoundError:
    try:
        import tomli as tomllib  # py310-
    except Exception:
        tomllib = None


def log(msg: str) -> None:
    now = dt.datetime.now().strftime("%H:%M:%S")
    print(f"[{now}] {msg}", flush=True)


def mask_key_tail(key: Optional[str]) -> str:
    if not key:
        return "(empty)"
    tail = key[-5:] if len(key) >= 5 else key
    return "*" * max(0, len(key) - 5) + tail


def load_config(path: str) -> dict:
    if not os.path.exists(path):
        raise FileNotFoundError(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {path}")
    if tomllib is None:
        raise RuntimeError("ç¼ºå°‘ tomllib/tomliï¼Œè¯·å®‰è£… tomli æˆ–ä½¿ç”¨ Python 3.11+")
    with open(path, "rb") as f:
        return tomllib.load(f)


def merge_llm_config(cfg: dict, llm_name: str, cli_api_key: Optional[str]) -> dict:
    base = cfg.get("llm", {}) or {}
    # TOML ä¸­ [llm.gemini_search] ä¼šè¢«è§£æä¸ºåµŒå¥—è¡¨ cfg["llm"]["gemini_search"]
    # æ”¯æŒä¸¤ç§å†™æ³•ï¼š
    # 1) [llm] + [llm.gemini_search] - æ ‡å‡†åµŒå¥—è¡¨
    # 2) cfg.get("llm.gemini_search", {}) - å®¹é”™ï¼ˆæŸäº›è§£æå™¨å¯èƒ½æ”¯æŒï¼‰
    llm_section = cfg.get("llm", {}) or {}
    if isinstance(llm_section, dict):
        by_table = llm_section.get(llm_name, {})  # æ ‡å‡†æ–¹å¼ï¼šcfg["llm"]["gemini_search"]
    else:
        by_table = {}
    # å®¹é”™ï¼šå°è¯•ç›´æ¥é”®è®¿é—®ï¼ˆæŸäº› TOML è§£æå™¨å¯èƒ½æ”¯æŒï¼‰
    if not by_table:
        by_table = cfg.get(f"llm.{llm_name}", {}) or {}

    # åˆå¹¶ï¼šbase <- by_table
    merged = dict(base)
    if isinstance(by_table, dict):
        merged.update(by_table)

    # CLI api_key ä¼˜å…ˆ
    if cli_api_key:
        merged["api_key"] = cli_api_key

    # å¿…è¦å­—æ®µæ£€æŸ¥
    # api_base ä¸ºå¯é€‰ï¼Œå¦‚æœæä¾›åˆ™ç”¨äºè‡ªå®šä¹‰ API ç«¯ç‚¹
    api_base = merged.get("api_base")
    api_key = merged.get("api_key")
    model_id = merged.get("model_id")
    if not (api_key or merged.get("user_token")):
        raise ValueError("æœªæä¾› api_keyï¼ˆæˆ– user_tokenï¼‰")
    if not model_id:
        raise ValueError("é…ç½®ç¼ºå°‘ llm.model_id")

    # é»˜è®¤å¹¶å‘/é‡è¯•/è¶…æ—¶
    merged.setdefault("parallel", 5)
    merged.setdefault("retry_times", 1)
    merged.setdefault("retry_delay", 10)
    merged.setdefault("timeout", 120)
    # è”ç½‘æœç´¢åŠŸèƒ½ï¼ˆé»˜è®¤å…³é—­ï¼‰
    merged.setdefault("enable_google_search", False)
    return merged


def parse_rows_arg(rows_arg: Optional[str], data_start_row: int, data_end_row: int) -> List[int]:
    """
    rows è¯­æ³•ï¼š
      - None: å¤„ç† data_start_row..data_end_row
      - "2-5": å¤„ç† 2..5
      - "2+":  å¤„ç† 2..data_end_row
    è¿”å›ï¼šåŸå§‹è¡Œå·åˆ—è¡¨ï¼ˆåŸºäºå¯åŠ¨æ—¶çš„è¡Œå·ï¼‰
    """
    if not rows_arg:
        return list(range(data_start_row, data_end_row + 1))

    rows_arg = rows_arg.strip()
    m = re.fullmatch(r"(\d+)\-(\d+)", rows_arg)
    if m:
        a, b = int(m.group(1)), int(m.group(2))
        a = max(a, data_start_row)
        b = min(b, data_end_row)
        if a > b:
            return []
        return list(range(a, b + 1))

    m = re.fullmatch(r"(\d+)\+", rows_arg)
    if m:
        a = int(m.group(1))
        a = max(a, data_start_row)
        return list(range(a, data_end_row + 1))

    raise ValueError(f"rows å‚æ•°ä¸åˆæ³•: {rows_arg}")


def get_sheet(wb, name: str) -> Worksheet:
    if name not in wb.sheetnames:
        raise ValueError(f"Excel ç¼ºå°‘å·¥ä½œè¡¨: {name}")
    return wb[name]


def find_header_indexes(ws: Worksheet) -> Dict[str, int]:
    """
    æ‰«æç¬¬1è¡Œï¼Œè¿”å›ï¼šåˆ—å -> åˆ—ç´¢å¼•ï¼ˆ1-basedï¼‰
    """
    headers = {}
    for col in range(1, ws.max_column + 1):
        v = ws.cell(row=1, column=col).value
        if v is None:
            continue
        headers[str(v).strip()] = col
    return headers


def ensure_columns(ws: Worksheet, headers: Dict[str, int], need_cols: List[str]) -> Dict[str, int]:
    """
    ç¡®ä¿ need_cols å­˜åœ¨äºè¡¨å¤´ï¼Œä¸å­˜åœ¨åˆ™åœ¨æœ«å°¾è¿½åŠ ã€‚è¿”å›æ›´æ–°åçš„åˆ—æ˜ å°„ã€‚
    """
    updated = dict(headers)
    for name in need_cols:
        if name not in updated:
            ws.cell(row=1, column=ws.max_column + 1, value=name)
            updated[name] = ws.max_column  # åˆšå†™å…¥çš„å•å…ƒæ ¼å·²ç»ç”Ÿæ•ˆ
    return updated


def compact_preview(text: str, limit: int = 30) -> str:
    text = (text or "").replace("\n", " ").strip()
    return text if len(text) <= limit else text[:limit] + "..."


def is_json_array_text(s: str) -> bool:
    s = s.strip()
    return s.startswith("[") and s.endswith("]")


def extract_json_array_from_text(s: str) -> str:
    """
    å…¼å®¹æ¨¡å‹æŠŠ JSON æ”¾åœ¨ ```json ... ``` æˆ–å‰åæœ‰è¯´æ˜æ–‡å­—çš„æƒ…å†µã€‚
    ç­–ç•¥ï¼š
      1) å»é™¤ ```...``` åŒ…è£¹
      2) ä»æ–‡æœ¬ä¸­æ‰¾åˆ°æœ€å¤–å±‚æ–¹æ‹¬å·çš„ JSON æ®µ
    """
    text = s.strip()

    # å»é™¤ ```json ... ``` åŒ…è£¹
    fence = re.compile(r"^```(?:json|JSON)?\s*(.*?)\s*```$", re.S)
    m = fence.match(text)
    if m:
        text = m.group(1).strip()

    if is_json_array_text(text):
        return text

    # å®½æ¾ï¼šä»é¦–ä¸ª '[' åˆ°æœ€åä¸€ä¸ª ']' çš„åŒ…è£¹
    lb = text.find("[")
    rb = text.rfind("]")
    if lb != -1 and rb != -1 and rb > lb:
        candidate = text[lb:rb + 1].strip()
        if is_json_array_text(candidate):
            return candidate

    # å¤±è´¥åˆ™è¿”å›åŸæ–‡ï¼ˆè®©ä¸Šå±‚æŠ¥é”™ï¼‰
    return text


def call_llm_genai(
    client: genai.Client,
    model: str,
    system_prompt: str,
    user_content: str,
    timeout: int,
    tools: Optional[List[types.Tool]] = None,
    debug: bool = False,
) -> Tuple[Optional[List[Dict[str, Any]]], Dict[str, Any], Optional[str]]:
    """
    è°ƒç”¨ Google GenAI SDK çš„ generate_content æ¥å£ã€‚
    è¿”å›ï¼š(json_arrayæˆ–None, usageå­—å…¸, é”™è¯¯æ–‡æœ¬æˆ–None)
    
    å‚æ•°:
        client: GenAI å®¢æˆ·ç«¯
        model: æ¨¡å‹ ID
        system_prompt: ç³»ç»Ÿæç¤ºè¯
        user_content: ç”¨æˆ·å†…å®¹
        timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
        tools: å¯é€‰çš„å·¥å…·åˆ—è¡¨ï¼ˆå¦‚ Google Searchï¼‰ï¼Œç”¨äºå¯ç”¨è”ç½‘æœç´¢ç­‰åŠŸèƒ½
        debug: æ˜¯å¦å¯ç”¨è°ƒè¯•æ¨¡å¼ï¼Œæ‰“å°è¯·æ±‚å’Œå“åº”å†…å®¹
    
    æ³¨æ„ï¼štimeout å‚æ•°ä¿ç•™åœ¨å‡½æ•°ç­¾åä¸­ä»¥ä¿æŒæ¥å£ä¸€è‡´æ€§ï¼Œ
    ä½† Google GenAI SDK çš„ generate_content å¯èƒ½ä¸ç›´æ¥æ”¯æŒè¯¥å‚æ•°ã€‚
    è¶…æ—¶æ§åˆ¶å¯èƒ½éœ€è¦é€šè¿‡ Client é…ç½®æˆ–å…¶ä»–æ–¹å¼å®ç°ã€‚
    """
    try:
        # æ„å»ºé…ç½®å¯¹è±¡
        # æ–°ç‰ˆ SDK è¦æ±‚é€šè¿‡ GenerateContentConfig ä¼ é€’æ‰€æœ‰é…ç½®å‚æ•°
        config_kwargs = {}
        
        # æ·»åŠ  system_instruction
        if system_prompt:
            config_kwargs["system_instruction"] = system_prompt
        
        # æ·»åŠ  toolsï¼ˆå¦‚æœæä¾›ï¼‰
        if tools:
            config_kwargs["tools"] = tools
        
        # åˆ›å»ºé…ç½®å¯¹è±¡ï¼ˆå¦‚æœæœ‰ä»»ä½•é…ç½®ï¼‰
        config = types.GenerateContentConfig(**config_kwargs) if config_kwargs else None
        
        # è°ƒè¯•æ¨¡å¼ï¼šæ‰“å°è¯·æ±‚ä¿¡æ¯
        if debug:
            log("=" * 60)
            log("ğŸ“¤ API è¯·æ±‚è¯¦æƒ…")
            log("=" * 60)
            log(f"æ¨¡å‹: {model}")
            log(f"ç³»ç»Ÿæç¤º (å‰200å­—): {compact_preview(system_prompt, 200) if system_prompt else '(æ— )'}")
            log(f"ç”¨æˆ·å†…å®¹ (å‰200å­—): {compact_preview(user_content, 200)}")
            if tools:
                log(f"å·¥å…·: {[str(t) for t in tools]}")
            log("=" * 60)
        
        # è°ƒç”¨ API
        response = client.models.generate_content(
            model=model,
            contents=user_content,
            config=config
        )
        
    except Exception as e:
        if debug:
            log("=" * 60)
            log("âŒ è¯·æ±‚å¼‚å¸¸")
            log("=" * 60)
            log(f"é”™è¯¯: {type(e).__name__}: {e}")
            import traceback
            log(f"å †æ ˆ:\n{traceback.format_exc()}")
            log("=" * 60)
        return None, {}, f"è¯·æ±‚å¼‚å¸¸: {type(e).__name__}: {e}"

    # æå–å“åº”æ–‡æœ¬
    try:
        content = response.text
    except Exception as e:
        return None, {}, f"å“åº”ç¼ºå°‘ text å±æ€§: {type(e).__name__}: {e}"

    # è°ƒè¯•æ¨¡å¼ï¼šæ‰“å°å“åº”ä¿¡æ¯
    if debug:
        log("=" * 60)
        log("ğŸ“¥ API å“åº”è¯¦æƒ…")
        log("=" * 60)
        log(f"åŸå§‹å“åº” (å‰500å­—): {compact_preview(content, 500)}")
        
        # æ£€æŸ¥ grounding metadataï¼ˆè”ç½‘æœç´¢ä¿¡æ¯ï¼‰
        if hasattr(response, 'candidates') and response.candidates:
            candidate = response.candidates[0]
            if hasattr(candidate, 'grounding_metadata') and candidate.grounding_metadata:
                metadata = candidate.grounding_metadata
                log(f"ğŸŒ è”ç½‘æœç´¢ä¿¡æ¯:")
                if hasattr(metadata, 'web_search_queries') and metadata.web_search_queries:
                    log(f"  æœç´¢æŸ¥è¯¢: {metadata.web_search_queries}")
                if hasattr(metadata, 'grounding_chunks') and metadata.grounding_chunks:
                    log(f"  æœç´¢ç»“æœæ•°: {len(metadata.grounding_chunks)}")
                    for i, chunk in enumerate(metadata.grounding_chunks[:3], 1):
                        if hasattr(chunk, 'web') and chunk.web:
                            title = getattr(chunk.web, 'title', 'N/A')
                            uri = getattr(chunk.web, 'uri', 'N/A')
                            log(f"    {i}. {title}: {uri}")

    # æå– usage ä¿¡æ¯ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    usage = {}
    try:
        # å°è¯•å¤šç§å¯èƒ½çš„ usage å±æ€§è·¯å¾„
        if hasattr(response, 'usage_metadata') and response.usage_metadata:
            usage_meta = response.usage_metadata
            usage = {
                "prompt_tokens": getattr(usage_meta, 'prompt_token_count', 0) or 0,
                "completion_tokens": getattr(usage_meta, 'completion_token_count', 0) or 0,
                "total_tokens": getattr(usage_meta, 'total_token_count', 0) or 0,
            }
        elif hasattr(response, 'usage') and response.usage:
            # å…¼å®¹å…¶ä»–å¯èƒ½çš„ usage æ ¼å¼
            usage_obj = response.usage
            usage = {
                "prompt_tokens": getattr(usage_obj, 'prompt_tokens', 0) or getattr(usage_obj, 'input_tokens', 0) or 0,
                "completion_tokens": getattr(usage_obj, 'completion_tokens', 0) or getattr(usage_obj, 'output_tokens', 0) or 0,
                "total_tokens": getattr(usage_obj, 'total_tokens', 0) or 0,
            }
        
        if debug and usage:
            log(f"ğŸ“Š Token ä½¿ç”¨: prompt={usage.get('prompt_tokens', 0)}, "
                f"completion={usage.get('completion_tokens', 0)}, "
                f"total={usage.get('total_tokens', 0)}")
    except Exception:
        # å¦‚æœæ— æ³•æå– usageï¼Œç»§ç»­æ‰§è¡Œï¼ˆusage ä¸ºç©ºå­—å…¸ï¼‰
        pass

    # è§£æ JSON æ•°ç»„
    content = extract_json_array_from_text(str(content))
    
    if debug:
        log(f"æå–çš„ JSON (å‰500å­—): {compact_preview(content, 500)}")
    
    try:
        arr = json.loads(content)
    except Exception as e:
        if debug:
            log(f"âŒ JSON è§£æå¤±è´¥: {type(e).__name__}: {e}")
            log("=" * 60)
        return None, usage, f"å†…å®¹ä¸æ˜¯ JSON æ•°ç»„: {type(e).__name__}: {e}; åŸæ–‡ç‰‡æ®µ: {content[:1000]}"

    if not isinstance(arr, list):
        if debug:
            log(f"âŒ é¡¶å±‚ä¸æ˜¯æ•°ç»„ï¼Œè€Œæ˜¯: {type(arr)}")
            log("=" * 60)
        return None, usage, "é¡¶å±‚éæ•°ç»„"
    
    # å…ƒç´ å¿…é¡»ä¸ºå¯¹è±¡
    for i, it in enumerate(arr):
        if not isinstance(it, dict):
            if debug:
                log(f"âŒ æ•°ç»„ç¬¬ {i+1} ä¸ªå…ƒç´ ä¸æ˜¯å¯¹è±¡")
                log("=" * 60)
            return None, usage, f"æ•°ç»„ç¬¬ {i+1} ä¸ªå…ƒç´ ä¸æ˜¯å¯¹è±¡"
    
    if debug:
        log(f"âœ… æˆåŠŸè§£æ JSON æ•°ç»„ï¼ŒåŒ…å« {len(arr)} ä¸ªå…ƒç´ ")
        if arr:
            log(f"ç¬¬ä¸€ä¸ªå…ƒç´ çš„é”®: {list(arr[0].keys())}")
        log("=" * 60)
    
    return arr, usage, None


def with_retry(func, retry_times: int, retry_delay: int):
    def wrapper(*args, **kwargs):
        last_err = None
        for i in range(retry_times + 1):
            result = func(*args, **kwargs)
            # çº¦å®šï¼šfunc è¿”å› (arr, usage, err_text)
            if result[2] is None:
                return result
            last_err = result[2]
            # å¯¹å¯é‡è¯•é”™è¯¯åšç®€å•åˆ¤æ–­ï¼ˆå« 429/5xx æ–‡æœ¬æ—¶é€€é¿ï¼‰ï¼Œå¦åˆ™ä¹Ÿç®€å•ç­‰ä¸€ç­‰
            time.sleep(retry_delay if i < retry_times else 0)
        return (None, {}, last_err)
    return wrapper


def save_with_backup_atomic(wb, xlsx_path: str, made_backup: List[bool]) -> None:
    """
    é¦–æ¬¡ä¿å­˜å‰åš .bak å¤‡ä»½ï¼›ä½¿ç”¨ä¸´æ—¶æ–‡ä»¶ + æ›¿æ¢ çš„åŸºæœ¬åŸå­å†™æ³•
    """
    if not made_backup[0]:
        bak = xlsx_path + ".bak"
        if not os.path.exists(bak):
            try:
                with open(xlsx_path, "rb") as rf, open(bak, "wb") as wf:
                    wf.write(rf.read())
                log(f"å·²åˆ›å»ºå¤‡ä»½: {bak}")
            except Exception as e:
                log(f"åˆ›å»ºå¤‡ä»½å¤±è´¥ï¼ˆå¿½ç•¥ï¼‰: {e}")
        made_backup[0] = True

    tmp = xlsx_path + ".tmp"
    wb.save(tmp)
    # Windows ä¸‹æ›¿æ¢
    try:
        if os.path.exists(xlsx_path):
            os.replace(tmp, xlsx_path)
        else:
            os.rename(tmp, xlsx_path)
    except Exception as e:
        log(f"ä¿å­˜æ›¿æ¢å¤±è´¥: {e}")
        # å…œåº•ç›´æ¥å†™åŸæ–‡ä»¶ï¼ˆå¯èƒ½å¤±è´¥ï¼‰
        wb.save(xlsx_path)


def main():
    parser = argparse.ArgumentParser(description="æ‰¹é‡è°ƒç”¨ LLM å¹¶å†™å› Excelï¼ˆGoogle GenAI SDK ç‰ˆæœ¬ï¼‰")
    parser.add_argument("--input-file", required=True, help="è¾“å…¥ Excel æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--config", default="config.toml", help="é…ç½®æ–‡ä»¶è·¯å¾„ï¼Œé»˜è®¤ config.toml")
    parser.add_argument("--llm", required=True, help="ä½¿ç”¨çš„æ¨¡å‹é…ç½®åï¼Œä¾‹å¦‚ genai_2_5_flash_latest")
    parser.add_argument("--rows", default=None, help="å¤„ç†è¡ŒèŒƒå›´ï¼Œä¾‹å¦‚ 2-5 æˆ– 2+ï¼›ç¼ºçœå¤„ç†å…¨éƒ¨")
    parser.add_argument("--api-key", default=None, help="å¯é€‰ï¼›å‘½ä»¤è¡Œè¦†ç›–é…ç½®ä¸­çš„ api_key")
    parser.add_argument("--debug", action="store_true", help="å¯ç”¨è°ƒè¯•æ¨¡å¼ï¼Œè¾“å‡ºè¯¦ç»†æ—¥å¿—")
    args = parser.parse_args()
    
    # å¦‚æœå¯ç”¨è°ƒè¯•æ¨¡å¼ï¼Œé…ç½®æ—¥å¿—
    if args.debug:
        import logging
        logging.basicConfig(
            level=logging.DEBUG,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        # ä¸ºç›¸å…³çš„ logger è®¾ç½® DEBUG çº§åˆ«
        for logger_name in ['google', 'google_genai', 'httpx', 'httpcore']:
            logging.getLogger(logger_name).setLevel(logging.DEBUG)
        log("å·²å¯ç”¨è°ƒè¯•æ¨¡å¼")

    xlsx_path = args.input_file
    if not os.path.exists(xlsx_path):
        print(f"æ‰¾ä¸åˆ°è¾“å…¥æ–‡ä»¶: {xlsx_path}", file=sys.stderr)
        sys.exit(2)

    # è¯»é…ç½®
    cfg = load_config(args.config)
    llm_cfg = merge_llm_config(cfg, args.llm, args.api_key)

    api_key = llm_cfg.get("api_key") or ""
    model_id = llm_cfg["model_id"]
    parallel = int(llm_cfg.get("parallel", 5))
    retry_times = int(llm_cfg.get("retry_times", 1))
    retry_delay = int(llm_cfg.get("retry_delay", 10))
    timeout = int(llm_cfg.get("timeout", 120))
    price_in = float(llm_cfg.get("price_per_1m_input_tokens", 0.0))
    price_out = float(llm_cfg.get("price_per_1m_output_tokens", 0.0))

    api_base = llm_cfg.get("api_base")
    # ç©ºå­—ç¬¦ä¸²è§†ä¸ºæœªè®¾ç½®
    if api_base is not None and str(api_base).strip() == "":
        api_base = None
    
    enable_google_search = bool(llm_cfg.get("enable_google_search", False))
    
    log("å¯åŠ¨å‚æ•°ï¼š")
    log(f"- input-file: {xlsx_path}")
    log(f"- llm: {args.llm}")
    log(f"- model_id: {model_id}")
    log(f"- api_key: {mask_key_tail(api_key)}")
    if api_base:
        log(f"- api_base: {api_base}")
    else:
        log(f"- api_base: (ä½¿ç”¨é»˜è®¤ Google API)")
    log(f"- parallel: {parallel}, retry_times: {retry_times}, retry_delay: {retry_delay}s, timeout: {timeout}s")
    log(f"- enable_google_search: {enable_google_search}")
    if args.rows:
        log(f"- rows: {args.rows}")

    # åˆ›å»º Google GenAI å®¢æˆ·ç«¯
    try:
        # å¦‚æœæä¾›äº† api_baseï¼Œä½¿ç”¨ http_options è‡ªå®šä¹‰ç«¯ç‚¹
        if api_base:
            client = genai.Client(
                api_key=api_key,
                http_options=types.HttpOptions(base_url=api_base)
            )
        else:
            client = genai.Client(api_key=api_key)
    except Exception as e:
        print(f"æ— æ³•åˆ›å»º GenAI å®¢æˆ·ç«¯ï¼š{e}", file=sys.stderr)
        sys.exit(2)

    # åˆ›å»ºå·¥å…·ï¼ˆå¦‚æœå¯ç”¨è”ç½‘æœç´¢ï¼‰
    tools = None
    if enable_google_search:
        try:
            # åˆ›å»º Google Search å·¥å…·ï¼ˆä½¿ç”¨ google_search è€Œä¸æ˜¯ google_search_retrievalï¼‰
            # API è¦æ±‚ä½¿ç”¨ google_searchï¼Œè€Œä¸æ˜¯å·²å¼ƒç”¨çš„ google_search_retrieval
            google_search = types.GoogleSearch()
            google_search_tool = types.Tool(google_search=google_search)
            tools = [google_search_tool]
            log("âœ“ å·²å¯ç”¨ Google è”ç½‘æœç´¢åŠŸèƒ½")
        except Exception as e:
            log(f"âš  åˆ›å»º Google Search å·¥å…·å¤±è´¥: {e}ï¼Œå°†ä¸ä½¿ç”¨è”ç½‘æœç´¢")
            tools = None

    # è¯» Excel
    try:
        wb = load_workbook(xlsx_path)
    except Exception as e:
        print(f"æ— æ³•æ‰“å¼€ Excelï¼š{e}", file=sys.stderr)
        sys.exit(2)

    ws_prompt = get_sheet(wb, "prompt")
    ws_qa = get_sheet(wb, "QA")

    # ç³»ç»Ÿæç¤ºï¼ˆprompt!A1ï¼‰
    sys_prompt = ws_prompt["A1"].value
    if sys_prompt is None or str(sys_prompt).strip() == "":
        print("prompt!A1 ä¸èƒ½ä¸ºç©º", file=sys.stderr)
        sys.exit(2)
    sys_prompt = str(sys_prompt)

    # QA è¡¨å¤´
    headers = find_header_indexes(ws_qa)
    if "Q" not in headers:
        print("QA é¡µç¼ºå°‘è¡¨å¤´åˆ—ï¼šQ", file=sys.stderr)
        sys.exit(2)
    # ç¡®ä¿å¿…è¦åˆ—
    headers = ensure_columns(ws_qa, headers, [COL_FOUND, COL_ERROR])
    col_Q = headers["Q"]
    col_found = headers[COL_FOUND]
    col_err = headers[COL_ERROR]

    # è¾“å‡ºå­—æ®µé›†åˆï¼šè¡¨å¤´ä¸­é™¤å» Q/æ˜¯å¦æ‰¾åˆ°/é”™è¯¯ çš„å…¶å®ƒåˆ—ï¼ˆä»…å†™è¿™äº›ï¼‰
    output_cols = {k: v for k, v in headers.items() if k not in ("Q", COL_FOUND, COL_ERROR)}

    data_start_row = 2
    data_end_row_initial = ws_qa.max_row  # å¯åŠ¨æ—¶çš„åŸå§‹æœ«è¡Œï¼ˆç”¨äº rows èŒƒå›´ï¼‰
    target_rows = parse_rows_arg(args.rows, data_start_row, data_end_row_initial)

    # ç»Ÿè®¡å»é‡ Qï¼šå…¨éƒ¨å€™é€‰ + å·²å®Œæˆ
    q_all = []
    q_done_set = set()
    for r in target_rows:
        qv = ws_qa.cell(row=r, column=col_Q).value
        if qv is None or str(qv).strip() == "":
            continue
        q_all.append(str(qv))
        found_v = ws_qa.cell(row=r, column=col_found).value
        if found_v is not None and str(found_v).strip() != "":
            q_done_set.add(str(qv))
    q_all_unique = set(q_all)
    log(f"å€™é€‰ Q å»é‡ç»Ÿè®¡ï¼šæ€» {len(q_all_unique)}ï¼Œå…¶ä¸­å·²å®Œæˆ {len(q_done_set)}")

    # ä¸º rows èŒƒå›´æ‰§è¡Œæ’å…¥åç§»è·Ÿè¸ªï¼šè®°å½•"åŸå§‹ä¸»è¡Œ" -> æ’å…¥çš„é¢å¤–è¡Œæ•°
    inserted_below: Dict[int, int] = {}

    made_backup = [False]

    # ç®€å•çš„è¿›åº¦ç´¯è®¡
    total = len(target_rows)
    n_done = 0
    n_success = 0  # æœ‰ç»“æœ
    n_empty = 0    # æ•°ç»„ç©º
    n_error = 0

    # è´¹ç”¨ç´¯è®¡ï¼ˆå½“ usage å­˜åœ¨æ—¶ï¼‰
    sum_prompt_tokens = 0
    sum_completion_tokens = 0

    retry_call = with_retry(
        lambda *a, **kw: call_llm_genai(*a, **kw),
        retry_times=retry_times,
        retry_delay=retry_delay,
    )

    def current_row_pos(original_row: int) -> int:
        """æ ¹æ®å·²æ’å…¥æƒ…å†µï¼Œè®¡ç®—è¯¥åŸå§‹è¡Œç°åœ¨çš„å®é™…è¡Œå·"""
        shift = 0
        for r0, added in inserted_below.items():
            if r0 < original_row:
                shift += added
        return original_row + shift

    for idx, r0 in enumerate(target_rows, start=1):
        r = current_row_pos(r0)
        qv = ws_qa.cell(row=r, column=col_Q).value
        qtext = "" if qv is None else str(qv).strip()

        # åˆ¤å®šæ˜¯å¦è·³è¿‡ï¼ˆæ–­ç‚¹ç»­è·‘ï¼šä¸»è¡Œ æ˜¯å¦æ‰¾åˆ° éç©ºå°±è·³è¿‡ï¼‰
        found_val = ws_qa.cell(row=r, column=col_found).value
        if found_val is not None and str(found_val).strip() != "":
            n_done += 1
            log(f"{idx}/{total} è·³è¿‡ï¼ˆå·²å®Œæˆï¼‰ r={r} Q='{compact_preview(qtext)}'")
            continue

        if qtext == "":
            # ç©º Qï¼šæ ‡è®°é”™è¯¯å¹¶ç»§ç»­
            ws_qa.cell(row=r, column=col_found, value="é”™è¯¯")
            ws_qa.cell(row=r, column=col_err, value="Q ä¸ºç©º")
            save_with_backup_atomic(wb, xlsx_path, made_backup)
            n_done += 1
            n_error += 1
            log(f"{idx}/{total} é”™è¯¯ï¼šQ ä¸ºç©ºï¼ˆr={r}ï¼‰ï¼Œå·²è½ç›˜")
            continue

        # è¯·æ±‚
        arr, usage, err = retry_call(
            client, model_id, sys_prompt, qtext, timeout, tools, args.debug
        )

        if usage:
            sum_prompt_tokens += int(usage.get("prompt_tokens", 0))
            sum_completion_tokens += int(usage.get("completion_tokens", 0))

        if err is not None:
            # å†™å…¥ä¸»è¡Œé”™è¯¯
            ws_qa.cell(row=r, column=col_found, value="é”™è¯¯")
            ws_qa.cell(row=r, column=col_err, value=str(err)[:500])
            save_with_backup_atomic(wb, xlsx_path, made_backup)
            n_done += 1
            n_error += 1
            log(f"{idx}/{total} é”™è¯¯ r={r} Q='{compact_preview(qtext)}' -> {err}")
            continue

        # arr ä¸€å®šæ˜¯ list[dict]
        if len(arr) == 0:
            # æ— ç»“æœï¼šä¸»è¡Œå†™"å¦"ï¼Œä¸æ’å…¥æ–°è¡Œ
            ws_qa.cell(row=r, column=col_found, value="å¦")
            ws_qa.cell(row=r, column=col_err, value="")
            # æ¸…ç©ºè¾“å‡ºåˆ—
            for name, c in output_cols.items():
                ws_qa.cell(row=r, column=c, value="")
            save_with_backup_atomic(wb, xlsx_path, made_backup)
            inserted_below[r0] = 0
            n_done += 1
            n_empty += 1
            log(f"{idx}/{total} ç©ºç»“æœ r={r} Q='{compact_preview(qtext)}'ï¼ˆå·²è½ç›˜ï¼‰")
            continue

        # æœ‰ç»“æœï¼šä¸»è¡Œå†™ç¬¬1ä¸ªï¼Œä¸‹é¢æ’å…¥ len(arr)-1 è¡Œå†™å…¶ä½™
        # æ‰€æœ‰å±•å¼€è¡Œçš„ Q ä¸ æ˜¯å¦æ‰¾åˆ° ç›¸åŒ
        ws_qa.cell(row=r, column=col_Q, value=qtext)
        ws_qa.cell(row=r, column=col_found, value="æ˜¯")
        ws_qa.cell(row=r, column=col_err, value="")
        # å†™è¾“å‡ºå­—æ®µ
        first_obj = arr[0]
        for name, c in output_cols.items():
            v = first_obj.get(name, "")
            if isinstance(v, (dict, list)):
                v = json.dumps(v, ensure_ascii=False)
            ws_qa.cell(row=r, column=c, value=v)

        extra = max(0, len(arr) - 1)
        if extra > 0:
            ws_qa.insert_rows(r + 1, amount=extra)
            # é€æ¡å†™å…¥
            for i in range(extra):
                rr = r + 1 + i
                ws_qa.cell(row=rr, column=col_Q, value=qtext)
                ws_qa.cell(row=rr, column=col_found, value="æ˜¯")
                ws_qa.cell(row=rr, column=col_err, value="")
                obj = arr[1 + i]
                for name, c in output_cols.items():
                    v = obj.get(name, "")
                    if isinstance(v, (dict, list)):
                        v = json.dumps(v, ensure_ascii=False)
                    ws_qa.cell(row=rr, column=c, value=v)

        inserted_below[r0] = extra
        save_with_backup_atomic(wb, xlsx_path, made_backup)
        n_done += 1
        n_success += 1
        log(f"{idx}/{total} æˆåŠŸ r={r} Q='{compact_preview(qtext)}' å±•å¼€ {len(arr)} è¡Œï¼ˆå·²è½ç›˜ï¼‰")

    # ç»“æŸç»Ÿè®¡
    cost = 0.0
    if price_in or price_out:
        cost = (sum_prompt_tokens / 1_000_000.0) * price_in + (sum_completion_tokens / 1_000_000.0) * price_out

    log("å®Œæˆã€‚")
    log(f"- æ€»è®¡ï¼š{total}, æˆåŠŸ(æœ‰ç»“æœ)={n_success}, ç©ºç»“æœ={n_empty}, é”™è¯¯={n_error}")
    if (sum_prompt_tokens + sum_completion_tokens) > 0:
        log(f"- tokens: prompt={sum_prompt_tokens}, completion={sum_completion_tokens}, ä¼°ç®—è´¹ç”¨=${cost:.4f}ï¼ˆæŒ‰é…ç½®å•ä»·ï¼‰")


if __name__ == "__main__":
    main()

