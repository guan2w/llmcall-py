#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
LLM æ‰¹é‡è°ƒç”¨ï¼šè¯»å– Excelï¼ˆprompt & QAï¼‰ï¼ŒæŒ‰ QA çš„ Q åˆ—é€è¡Œè¯·æ±‚ LLMï¼Œ
å°† JSON æ•°ç»„ç»“æœå±•å¼€å†™å›åŸæ–‡ä»¶ã€‚æ»¡è¶³ï¼š
- å±•å¼€ç»“æœçš„æ‰€æœ‰è¡Œï¼šQ ä¸ æ˜¯å¦æ‰¾åˆ° ç›¸åŒ
- æ¯ä¸ªè¾“å…¥å¤„ç†å®Œæˆåç«‹å³è½ç›˜
- æ”¯æŒ rows èŒƒå›´ã€æ–­ç‚¹ç»­è·‘ã€å¹¶å‘è¯·æ±‚ï¼ˆè¯·æ±‚å¹¶å‘ï¼Œå†™å…¥ä¸²è¡Œï¼‰

æœ¬ç‰ˆæœ¬ä½¿ç”¨ Google GenAI SDK (google-genai)
"""

import argparse
import datetime as dt
import json
import os
import re
import sys
import time
from typing import Any, Dict, List, Optional, Tuple

from google import genai
from google.genai import types
from openpyxl import load_workbook
from openpyxl.worksheet.worksheet import Worksheet

# --- åˆ—åé…ç½® ---
COL_FOUND = "FOUND"  # ç»“æœçŠ¶æ€åˆ—å
COL_ERROR = "ERROR"      # é”™è¯¯ä¿¡æ¯åˆ—å

# --- é…ç½®è§£æï¼ˆtomllib ä¼˜å…ˆï¼‰ ---
try:
    import tomllib  # py311+
except ModuleNotFoundError:
    try:
        import tomli as tomllib  # py310-
    except Exception:
        tomllib = None


def log(msg: str) -> None:
    now = dt.datetime.now().strftime("%H:%M:%S")
    print(f"[{now}] {msg}", flush=True)


def mask_key_tail(key: Optional[str]) -> str:
    if not key:
        return "(empty)"
    tail = key[-5:] if len(key) >= 5 else key
    return "*" * max(0, len(key) - 5) + tail


def load_config(path: str) -> dict:
    if not os.path.exists(path):
        raise FileNotFoundError(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {path}")
    if tomllib is None:
        raise RuntimeError("ç¼ºå°‘ tomllib/tomliï¼Œè¯·å®‰è£… tomli æˆ–ä½¿ç”¨ Python 3.11+")
    with open(path, "rb") as f:
        return tomllib.load(f)


def merge_llm_config(cfg: dict, llm_name: str, cli_api_key: Optional[str]) -> dict:
    base = cfg.get("llm", {}) or {}
    # TOML ä¸­ [llm.gemini_search] ä¼šè¢«è§£æä¸ºåµŒå¥—è¡¨ cfg["llm"]["gemini_search"]
    # æ”¯æŒä¸¤ç§å†™æ³•ï¼š
    # 1) [llm] + [llm.gemini_search] - æ ‡å‡†åµŒå¥—è¡¨
    # 2) cfg.get("llm.gemini_search", {}) - å®¹é”™ï¼ˆæŸäº›è§£æå™¨å¯èƒ½æ”¯æŒï¼‰
    llm_section = cfg.get("llm", {}) or {}
    if isinstance(llm_section, dict):
        by_table = llm_section.get(llm_name, {})  # æ ‡å‡†æ–¹å¼ï¼šcfg["llm"]["gemini_search"]
    else:
        by_table = {}
    # å®¹é”™ï¼šå°è¯•ç›´æ¥é”®è®¿é—®ï¼ˆæŸäº› TOML è§£æå™¨å¯èƒ½æ”¯æŒï¼‰
    if not by_table:
        by_table = cfg.get(f"llm.{llm_name}", {}) or {}

    # åˆå¹¶ï¼šbase <- by_table
    merged = dict(base)
    if isinstance(by_table, dict):
        merged.update(by_table)

    # CLI api_key ä¼˜å…ˆ
    if cli_api_key:
        merged["api_key"] = cli_api_key

    # å¿…è¦å­—æ®µæ£€æŸ¥
    # api_base ä¸ºå¯é€‰ï¼Œå¦‚æœæä¾›åˆ™ç”¨äºè‡ªå®šä¹‰ API ç«¯ç‚¹
    api_base = merged.get("api_base")
    api_key = merged.get("api_key")
    model_id = merged.get("model_id")
    if not (api_key or merged.get("user_token")):
        raise ValueError("æœªæä¾› api_keyï¼ˆæˆ– user_tokenï¼‰")
    if not model_id:
        raise ValueError("é…ç½®ç¼ºå°‘ llm.model_id")

    # é»˜è®¤å¹¶å‘/é‡è¯•/è¶…æ—¶
    merged.setdefault("parallel", 5)
    merged.setdefault("retry_times", 1)
    merged.setdefault("retry_delay", 10)
    merged.setdefault("timeout", 120)
    
    # è”ç½‘æœç´¢åŠŸèƒ½ï¼ˆé»˜è®¤å…³é—­ï¼‰
    merged.setdefault("enable_google_search", False)
    
    # URL Context åŠŸèƒ½ï¼ˆé»˜è®¤å¼€å¯ï¼‰
    merged.setdefault("enable_url_context", True)
    
    # ç”Ÿæˆå‚æ•°ï¼ˆå¯é€‰ï¼‰
    # temperature: æ§åˆ¶éšæœºæ€§ï¼Œ0.0-2.0ï¼Œé»˜è®¤ä¸è®¾ç½®ï¼ˆä½¿ç”¨æ¨¡å‹é»˜è®¤å€¼ï¼‰
    # thinking_budget: æ€è€ƒé¢„ç®—ï¼Œ-1 è¡¨ç¤ºæ— é™åˆ¶ï¼Œé»˜è®¤ä¸è®¾ç½®
    # è¿™äº›å‚æ•°å¦‚æœåœ¨é…ç½®ä¸­æœªè®¾ç½®ï¼Œåˆ™ä¸ä¼ é€’ç»™ APIï¼ˆä½¿ç”¨ API é»˜è®¤å€¼ï¼‰
    
    return merged


def parse_rows_arg(rows_arg: Optional[str], data_start_row: int, data_end_row: int) -> List[int]:
    """
    rows è¯­æ³•ï¼š
      - None: å¤„ç† data_start_row..data_end_row
      - "2-5": å¤„ç† 2..5
      - "2+":  å¤„ç† 2..data_end_row
    è¿”å›ï¼šåŸå§‹è¡Œå·åˆ—è¡¨ï¼ˆåŸºäºå¯åŠ¨æ—¶çš„è¡Œå·ï¼‰
    """
    if not rows_arg:
        return list(range(data_start_row, data_end_row + 1))

    rows_arg = rows_arg.strip()
    m = re.fullmatch(r"(\d+)\-(\d+)", rows_arg)
    if m:
        a, b = int(m.group(1)), int(m.group(2))
        a = max(a, data_start_row)
        b = min(b, data_end_row)
        if a > b:
            return []
        return list(range(a, b + 1))

    m = re.fullmatch(r"(\d+)\+", rows_arg)
    if m:
        a = int(m.group(1))
        a = max(a, data_start_row)
        return list(range(a, data_end_row + 1))

    raise ValueError(f"rows å‚æ•°ä¸åˆæ³•: {rows_arg}")


def get_sheet(wb, name: str) -> Worksheet:
    if name not in wb.sheetnames:
        raise ValueError(f"Excel ç¼ºå°‘å·¥ä½œè¡¨: {name}")
    return wb[name]


def find_header_indexes(ws: Worksheet) -> Dict[str, int]:
    """
    æ‰«æç¬¬1è¡Œï¼Œè¿”å›ï¼šåˆ—å -> åˆ—ç´¢å¼•ï¼ˆ1-basedï¼‰
    """
    headers = {}
    for col in range(1, ws.max_column + 1):
        v = ws.cell(row=1, column=col).value
        if v is None:
            continue
        headers[str(v).strip()] = col
    return headers


def ensure_columns(ws: Worksheet, headers: Dict[str, int], need_cols: List[str]) -> Dict[str, int]:
    """
    ç¡®ä¿ need_cols å­˜åœ¨äºè¡¨å¤´ï¼Œä¸å­˜åœ¨åˆ™åœ¨æœ«å°¾è¿½åŠ ã€‚è¿”å›æ›´æ–°åçš„åˆ—æ˜ å°„ã€‚
    """
    updated = dict(headers)
    for name in need_cols:
        if name not in updated:
            ws.cell(row=1, column=ws.max_column + 1, value=name)
            updated[name] = ws.max_column  # åˆšå†™å…¥çš„å•å…ƒæ ¼å·²ç»ç”Ÿæ•ˆ
    return updated


def compact_preview(text: str, limit: int = 30) -> str:
    text = (text or "").replace("\n", " ").strip()
    return text if len(text) <= limit else text[:limit] + "..."


def extract_template_vars(template: str) -> List[str]:
    """
    ä»æ¨¡æ¿ä¸­æå–æ‰€æœ‰ {{å˜é‡å}} å ä½ç¬¦ï¼Œè¿”å›å»é‡åçš„å˜é‡ååˆ—è¡¨ã€‚
    ä¾‹å¦‚ï¼š"æŸ¥æ‰¾{{å­¦æ ¡åç§°}}åœ¨{{åŸå¸‚}}çš„ä¿¡æ¯" -> ["å­¦æ ¡åç§°", "åŸå¸‚"]
    """
    pattern = r'\{\{([^}]+)\}\}'
    matches = re.findall(pattern, template)
    # å»é™¤ç©ºæ ¼å¹¶å»é‡ï¼Œä¿æŒé¡ºåº
    seen = set()
    result = []
    for m in matches:
        name = m.strip()
        if name and name not in seen:
            seen.add(name)
            result.append(name)
    return result


def fill_template(template: str, values: Dict[str, Any]) -> str:
    """
    ç”¨å­—å…¸å€¼å¡«å……æ¨¡æ¿ä¸­çš„ {{å˜é‡å}} å ä½ç¬¦ã€‚
    ä¾‹å¦‚ï¼štemplate="æŸ¥æ‰¾{{å­¦æ ¡åç§°}}çš„ä¿¡æ¯", values={"å­¦æ ¡åç§°": "åŒ—äº¬ä¸€ä¸­"} 
         -> "æŸ¥æ‰¾åŒ—äº¬ä¸€ä¸­çš„ä¿¡æ¯"
    """
    result = template
    for key, val in values.items():
        placeholder = f"{{{{{key}}}}}"
        result = result.replace(placeholder, str(val))
    return result


def is_empty_value(val: Any) -> bool:
    """
    åˆ¤æ–­æ˜¯å¦ä¸ºç©ºå€¼ï¼šNoneã€ç©ºå­—ç¬¦ä¸²ã€çº¯ç©ºæ ¼
    """
    if val is None:
        return True
    if isinstance(val, str) and val.strip() == "":
        return True
    return False


def is_json_array_text(s: str) -> bool:
    s = s.strip()
    return s.startswith("[") and s.endswith("]")


def extract_json_array_from_text(s: str) -> str:
    """
    å…¼å®¹æ¨¡å‹æŠŠ JSON æ”¾åœ¨ ```json ... ``` æˆ–å‰åæœ‰è¯´æ˜æ–‡å­—çš„æƒ…å†µã€‚
    ç­–ç•¥ï¼š
      1) å»é™¤ ```...``` åŒ…è£¹
      2) ä»æ–‡æœ¬ä¸­æ‰¾åˆ°æœ€å¤–å±‚æ–¹æ‹¬å·çš„ JSON æ®µ
    """
    text = s.strip()

    # å»é™¤ ```json ... ``` åŒ…è£¹
    fence = re.compile(r"^```(?:json|JSON)?\s*(.*?)\s*```$", re.S)
    m = fence.match(text)
    if m:
        text = m.group(1).strip()

    if is_json_array_text(text):
        return text

    # å®½æ¾ï¼šä»é¦–ä¸ª '[' åˆ°æœ€åä¸€ä¸ª ']' çš„åŒ…è£¹
    lb = text.find("[")
    rb = text.rfind("]")
    if lb != -1 and rb != -1 and rb > lb:
        candidate = text[lb:rb + 1].strip()
        if is_json_array_text(candidate):
            return candidate

    # å¤±è´¥åˆ™è¿”å›åŸæ–‡ï¼ˆè®©ä¸Šå±‚æŠ¥é”™ï¼‰
    return text


def call_llm_genai(
    client: genai.Client,
    model: str,
    system_prompt: str,
    user_content: str,
    timeout: int,
    tools: Optional[List[types.Tool]] = None,
    temperature: Optional[float] = None,
    thinking_budget: Optional[int] = None,
    debug: bool = False,
) -> Tuple[Optional[List[Dict[str, Any]]], Dict[str, Any], Optional[str]]:
    """
    è°ƒç”¨ Google GenAI SDK çš„ generate_content æ¥å£ã€‚
    è¿”å›ï¼š(json_arrayæˆ–None, usageå­—å…¸, é”™è¯¯æ–‡æœ¬æˆ–None)
    
    å‚æ•°:
        client: GenAI å®¢æˆ·ç«¯
        model: æ¨¡å‹ ID
        system_prompt: ç³»ç»Ÿæç¤ºè¯
        user_content: ç”¨æˆ·å†…å®¹
        timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
        tools: å¯é€‰çš„å·¥å…·åˆ—è¡¨ï¼ˆå¦‚ Google Searchï¼‰ï¼Œç”¨äºå¯ç”¨è”ç½‘æœç´¢ç­‰åŠŸèƒ½
        temperature: æ¸©åº¦å‚æ•°ï¼Œæ§åˆ¶éšæœºæ€§ï¼ˆ0.0-2.0ï¼‰
        thinking_budget: æ€è€ƒé¢„ç®—ï¼Œ-1 è¡¨ç¤ºæ— é™åˆ¶
        debug: æ˜¯å¦å¯ç”¨è°ƒè¯•æ¨¡å¼ï¼Œæ‰“å°è¯·æ±‚å’Œå“åº”å†…å®¹
    
    æ³¨æ„ï¼štimeout å‚æ•°ä¿ç•™åœ¨å‡½æ•°ç­¾åä¸­ä»¥ä¿æŒæ¥å£ä¸€è‡´æ€§ï¼Œ
    ä½† Google GenAI SDK çš„ generate_content å¯èƒ½ä¸ç›´æ¥æ”¯æŒè¯¥å‚æ•°ã€‚
    è¶…æ—¶æ§åˆ¶å¯èƒ½éœ€è¦é€šè¿‡ Client é…ç½®æˆ–å…¶ä»–æ–¹å¼å®ç°ã€‚
    """
    try:
        # æ„å»ºé…ç½®å¯¹è±¡
        # æ–°ç‰ˆ SDK è¦æ±‚é€šè¿‡ GenerateContentConfig ä¼ é€’æ‰€æœ‰é…ç½®å‚æ•°
        config_kwargs = {}
        
        # æ·»åŠ  system_instruction
        if system_prompt:
            config_kwargs["system_instruction"] = system_prompt
        
        # æ·»åŠ  toolsï¼ˆå¦‚æœæä¾›ï¼‰
        if tools:
            config_kwargs["tools"] = tools
        
        # æ·»åŠ  temperatureï¼ˆå¦‚æœæä¾›ï¼‰
        if temperature is not None:
            config_kwargs["temperature"] = temperature
        
        # æ·»åŠ  thinking_configï¼ˆå¦‚æœæä¾›ï¼‰
        if thinking_budget is not None:
            config_kwargs["thinking_config"] = types.ThinkingConfig(
                thinking_budget=thinking_budget
            )
        
        # åˆ›å»ºé…ç½®å¯¹è±¡ï¼ˆå¦‚æœæœ‰ä»»ä½•é…ç½®ï¼‰
        config = types.GenerateContentConfig(**config_kwargs) if config_kwargs else None
        
        # è°ƒè¯•æ¨¡å¼ï¼šæ‰“å°è¯·æ±‚ä¿¡æ¯
        if debug:
            log("=" * 60)
            log("ğŸ“¤ API è¯·æ±‚è¯¦æƒ…")
            log("=" * 60)
            log(f"æ¨¡å‹: {model}")
            log(f"ç³»ç»Ÿæç¤º (å‰200å­—): {compact_preview(system_prompt, 200) if system_prompt else '(æ— )'}")
            log(f"ç”¨æˆ·å†…å®¹ (å‰200å­—): {compact_preview(user_content, 200)}")
            if tools:
                log(f"å·¥å…·: {[str(t) for t in tools]}")
            log("=" * 60)
        
        # è°ƒç”¨ API
        response = client.models.generate_content(
            model=model,
            contents=user_content,
            config=config
        )
        
    except Exception as e:
        if debug:
            log("=" * 60)
            log("âŒ è¯·æ±‚å¼‚å¸¸")
            log("=" * 60)
            log(f"é”™è¯¯: {type(e).__name__}: {e}")
            import traceback
            log(f"å †æ ˆ:\n{traceback.format_exc()}")
            log("=" * 60)
        return None, {}, f"è¯·æ±‚å¼‚å¸¸: {type(e).__name__}: {e}"

    # æå–å“åº”æ–‡æœ¬
    try:
        content = response.text
    except Exception as e:
        return None, {}, f"å“åº”ç¼ºå°‘ text å±æ€§: {type(e).__name__}: {e}"

    # è°ƒè¯•æ¨¡å¼ï¼šæ‰“å°å“åº”ä¿¡æ¯
    if debug:
        log("=" * 60)
        log("ğŸ“¥ API å“åº”è¯¦æƒ…")
        log("=" * 60)
        log(f"åŸå§‹å“åº” (å‰500å­—): {compact_preview(content, 500)}")
        
        # æ£€æŸ¥ grounding metadataï¼ˆè”ç½‘æœç´¢ä¿¡æ¯ï¼‰
        if hasattr(response, 'candidates') and response.candidates:
            candidate = response.candidates[0]
            if hasattr(candidate, 'grounding_metadata') and candidate.grounding_metadata:
                metadata = candidate.grounding_metadata
                log(f"ğŸŒ è”ç½‘æœç´¢ä¿¡æ¯:")
                if hasattr(metadata, 'web_search_queries') and metadata.web_search_queries:
                    log(f"  æœç´¢æŸ¥è¯¢: {metadata.web_search_queries}")
                if hasattr(metadata, 'grounding_chunks') and metadata.grounding_chunks:
                    log(f"  æœç´¢ç»“æœæ•°: {len(metadata.grounding_chunks)}")
                    for i, chunk in enumerate(metadata.grounding_chunks[:3], 1):
                        if hasattr(chunk, 'web') and chunk.web:
                            title = getattr(chunk.web, 'title', 'N/A')
                            uri = getattr(chunk.web, 'uri', 'N/A')
                            log(f"    {i}. {title}: {uri}")

    # æå– usage ä¿¡æ¯ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    usage = {}
    try:
        # å°è¯•å¤šç§å¯èƒ½çš„ usage å±æ€§è·¯å¾„
        if hasattr(response, 'usage_metadata') and response.usage_metadata:
            usage_meta = response.usage_metadata
            usage = {
                "prompt_tokens": getattr(usage_meta, 'prompt_token_count', 0) or 0,
                "completion_tokens": getattr(usage_meta, 'completion_token_count', 0) or 0,
                "total_tokens": getattr(usage_meta, 'total_token_count', 0) or 0,
            }
        elif hasattr(response, 'usage') and response.usage:
            # å…¼å®¹å…¶ä»–å¯èƒ½çš„ usage æ ¼å¼
            usage_obj = response.usage
            usage = {
                "prompt_tokens": getattr(usage_obj, 'prompt_tokens', 0) or getattr(usage_obj, 'input_tokens', 0) or 0,
                "completion_tokens": getattr(usage_obj, 'completion_tokens', 0) or getattr(usage_obj, 'output_tokens', 0) or 0,
                "total_tokens": getattr(usage_obj, 'total_tokens', 0) or 0,
            }
        
        if debug and usage:
            log(f"ğŸ“Š Token ä½¿ç”¨: prompt={usage.get('prompt_tokens', 0)}, "
                f"completion={usage.get('completion_tokens', 0)}, "
                f"total={usage.get('total_tokens', 0)}")
    except Exception:
        # å¦‚æœæ— æ³•æå– usageï¼Œç»§ç»­æ‰§è¡Œï¼ˆusage ä¸ºç©ºå­—å…¸ï¼‰
        pass

    # è§£æ JSON æ•°ç»„
    content = extract_json_array_from_text(str(content))
    
    if debug:
        log(f"æå–çš„ JSON (å‰500å­—): {compact_preview(content, 500)}")
    
    try:
        arr = json.loads(content)
    except Exception as e:
        if debug:
            log(f"âŒ JSON è§£æå¤±è´¥: {type(e).__name__}: {e}")
            log("=" * 60)
        return None, usage, f"å†…å®¹ä¸æ˜¯ JSON æ•°ç»„: {type(e).__name__}: {e}; åŸæ–‡ç‰‡æ®µ: {content[:1000]}"

    if not isinstance(arr, list):
        if debug:
            log(f"âŒ é¡¶å±‚ä¸æ˜¯æ•°ç»„ï¼Œè€Œæ˜¯: {type(arr)}")
            log("=" * 60)
        return None, usage, "é¡¶å±‚éæ•°ç»„"
    
    # å…ƒç´ å¿…é¡»ä¸ºå¯¹è±¡
    for i, it in enumerate(arr):
        if not isinstance(it, dict):
            if debug:
                log(f"âŒ æ•°ç»„ç¬¬ {i+1} ä¸ªå…ƒç´ ä¸æ˜¯å¯¹è±¡")
                log("=" * 60)
            return None, usage, f"æ•°ç»„ç¬¬ {i+1} ä¸ªå…ƒç´ ä¸æ˜¯å¯¹è±¡"
    
    if debug:
        log(f"âœ… æˆåŠŸè§£æ JSON æ•°ç»„ï¼ŒåŒ…å« {len(arr)} ä¸ªå…ƒç´ ")
        if arr:
            log(f"ç¬¬ä¸€ä¸ªå…ƒç´ çš„é”®: {list(arr[0].keys())}")
        log("=" * 60)
    
    return arr, usage, None


def with_retry(func, retry_times: int, retry_delay: int):
    def wrapper(*args, **kwargs):
        last_err = None
        for i in range(retry_times + 1):
            result = func(*args, **kwargs)
            # çº¦å®šï¼šfunc è¿”å› (arr, usage, err_text)
            if result[2] is None:
                return result
            last_err = result[2]
            # å¯¹å¯é‡è¯•é”™è¯¯åšç®€å•åˆ¤æ–­ï¼ˆå« 429/5xx æ–‡æœ¬æ—¶é€€é¿ï¼‰ï¼Œå¦åˆ™ä¹Ÿç®€å•ç­‰ä¸€ç­‰
            time.sleep(retry_delay if i < retry_times else 0)
        return (None, {}, last_err)
    return wrapper


def save_with_backup_atomic(wb, xlsx_path: str, made_backup: List[bool]) -> None:
    """
    é¦–æ¬¡ä¿å­˜å‰åš .bak å¤‡ä»½ï¼›ä½¿ç”¨ä¸´æ—¶æ–‡ä»¶ + æ›¿æ¢ çš„åŸºæœ¬åŸå­å†™æ³•
    """
    if not made_backup[0]:
        bak = xlsx_path + ".bak"
        if not os.path.exists(bak):
            try:
                with open(xlsx_path, "rb") as rf, open(bak, "wb") as wf:
                    wf.write(rf.read())
                log(f"å·²åˆ›å»ºå¤‡ä»½: {bak}")
            except Exception as e:
                log(f"åˆ›å»ºå¤‡ä»½å¤±è´¥ï¼ˆå¿½ç•¥ï¼‰: {e}")
        made_backup[0] = True

    tmp = xlsx_path + ".tmp"
    wb.save(tmp)
    # Windows ä¸‹æ›¿æ¢
    try:
        if os.path.exists(xlsx_path):
            os.replace(tmp, xlsx_path)
        else:
            os.rename(tmp, xlsx_path)
    except Exception as e:
        log(f"ä¿å­˜æ›¿æ¢å¤±è´¥: {e}")
        # å…œåº•ç›´æ¥å†™åŸæ–‡ä»¶ï¼ˆå¯èƒ½å¤±è´¥ï¼‰
        wb.save(xlsx_path)


def main():
    parser = argparse.ArgumentParser(description="æ‰¹é‡è°ƒç”¨ LLM å¹¶å†™å› Excelï¼ˆGoogle GenAI SDK ç‰ˆæœ¬ï¼‰")
    parser.add_argument("--input-file", required=True, help="è¾“å…¥ Excel æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--config", default="config.toml", help="é…ç½®æ–‡ä»¶è·¯å¾„ï¼Œé»˜è®¤ config.toml")
    parser.add_argument("--llm", required=True, help="ä½¿ç”¨çš„æ¨¡å‹é…ç½®åï¼Œä¾‹å¦‚ genai_2_5_flash_latest")
    parser.add_argument("--rows", default=None, help="å¤„ç†è¡ŒèŒƒå›´ï¼Œä¾‹å¦‚ 2-5 æˆ– 2+ï¼›ç¼ºçœå¤„ç†å…¨éƒ¨")
    parser.add_argument("--api-key", default=None, help="å¯é€‰ï¼›å‘½ä»¤è¡Œè¦†ç›–é…ç½®ä¸­çš„ api_key")
    parser.add_argument("--debug", action="store_true", help="å¯ç”¨è°ƒè¯•æ¨¡å¼ï¼Œè¾“å‡ºè¯¦ç»†æ—¥å¿—")
    args = parser.parse_args()
    
    # å¦‚æœå¯ç”¨è°ƒè¯•æ¨¡å¼ï¼Œé…ç½®æ—¥å¿—
    if args.debug:
        import logging
        logging.basicConfig(
            level=logging.DEBUG,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        # ä¸ºç›¸å…³çš„ logger è®¾ç½® DEBUG çº§åˆ«
        for logger_name in ['google', 'google_genai', 'httpx', 'httpcore']:
            logging.getLogger(logger_name).setLevel(logging.DEBUG)
        log("å·²å¯ç”¨è°ƒè¯•æ¨¡å¼")

    xlsx_path = args.input_file
    if not os.path.exists(xlsx_path):
        print(f"æ‰¾ä¸åˆ°è¾“å…¥æ–‡ä»¶: {xlsx_path}", file=sys.stderr)
        sys.exit(2)

    # è¯»é…ç½®
    cfg = load_config(args.config)
    llm_cfg = merge_llm_config(cfg, args.llm, args.api_key)

    api_key = llm_cfg.get("api_key") or ""
    model_id = llm_cfg["model_id"]
    parallel = int(llm_cfg.get("parallel", 5))
    retry_times = int(llm_cfg.get("retry_times", 1))
    retry_delay = int(llm_cfg.get("retry_delay", 10))
    timeout = int(llm_cfg.get("timeout", 120))
    price_in = float(llm_cfg.get("price_per_1m_input_tokens", 0.0))
    price_out = float(llm_cfg.get("price_per_1m_output_tokens", 0.0))

    api_base = llm_cfg.get("api_base")
    # ç©ºå­—ç¬¦ä¸²è§†ä¸ºæœªè®¾ç½®
    if api_base is not None and str(api_base).strip() == "":
        api_base = None
    
    enable_google_search = bool(llm_cfg.get("enable_google_search", False))
    enable_url_context = bool(llm_cfg.get("enable_url_context", True))
    
    # ç”Ÿæˆå‚æ•°ï¼ˆå¯é€‰ï¼‰
    temperature = llm_cfg.get("temperature")  # None æˆ–æµ®ç‚¹æ•°
    if temperature is not None:
        temperature = float(temperature)
    
    thinking_budget = llm_cfg.get("thinking_budget")  # None æˆ–æ•´æ•°
    if thinking_budget is not None:
        thinking_budget = int(thinking_budget)
    
    log("å¯åŠ¨å‚æ•°ï¼š")
    log(f"- input-file: {xlsx_path}")
    log(f"- llm: {args.llm}")
    log(f"- model_id: {model_id}")
    log(f"- api_key: {mask_key_tail(api_key)}")
    if api_base:
        log(f"- api_base: {api_base}")
    else:
        log(f"- api_base: (ä½¿ç”¨é»˜è®¤ Google API)")
    log(f"- parallel: {parallel}, retry_times: {retry_times}, retry_delay: {retry_delay}s, timeout: {timeout}s")
    log(f"- enable_google_search: {enable_google_search}")
    log(f"- enable_url_context: {enable_url_context}")
    if temperature is not None:
        log(f"- temperature: {temperature}")
    if thinking_budget is not None:
        log(f"- thinking_budget: {thinking_budget}")
    if args.rows:
        log(f"- rows: {args.rows}")

    # åˆ›å»º Google GenAI å®¢æˆ·ç«¯
    try:
        # å¦‚æœæä¾›äº† api_baseï¼Œä½¿ç”¨ http_options è‡ªå®šä¹‰ç«¯ç‚¹
        if api_base:
            client = genai.Client(
                api_key=api_key,
                http_options=types.HttpOptions(base_url=api_base)
            )
        else:
            client = genai.Client(api_key=api_key)
    except Exception as e:
        print(f"æ— æ³•åˆ›å»º GenAI å®¢æˆ·ç«¯ï¼š{e}", file=sys.stderr)
        sys.exit(2)

    # åˆ›å»ºå·¥å…·ï¼ˆè”ç½‘æœç´¢å’Œ URL Contextï¼‰
    tools = None
    tools_list = []
    
    if enable_google_search:
        try:
            # åˆ›å»º Google Search å·¥å…·ï¼ˆä½¿ç”¨ google_search è€Œä¸æ˜¯ google_search_retrievalï¼‰
            # API è¦æ±‚ä½¿ç”¨ google_searchï¼Œè€Œä¸æ˜¯å·²å¼ƒç”¨çš„ google_search_retrieval
            google_search = types.GoogleSearch()
            google_search_tool = types.Tool(google_search=google_search)
            tools_list.append(google_search_tool)
            log("âœ“ å·²å¯ç”¨ Google è”ç½‘æœç´¢åŠŸèƒ½")
        except Exception as e:
            log(f"âš  åˆ›å»º Google Search å·¥å…·å¤±è´¥: {e}ï¼Œå°†ä¸ä½¿ç”¨è”ç½‘æœç´¢")
    
    if enable_url_context:
        try:
            # åˆ›å»º URL Context å·¥å…·ï¼Œå…è®¸æ¨¡å‹ç›´æ¥è®¿é—®å’Œç†è§£ç½‘é¡µå†…å®¹
            # æ³¨æ„ï¼šç±»åæ˜¯ UrlContextï¼ˆé©¼å³°å‘½åï¼‰ï¼Œä¸æ˜¯ URLContext
            url_context = types.UrlContext()
            url_context_tool = types.Tool(url_context=url_context)
            tools_list.append(url_context_tool)
            log("âœ“ å·²å¯ç”¨ URL Context åŠŸèƒ½ï¼ˆæ¨¡å‹å¯ç›´æ¥è®¿é—®ç½‘é¡µå†…å®¹ï¼‰")
        except Exception as e:
            log(f"âš  åˆ›å»º URL Context å·¥å…·å¤±è´¥: {e}ï¼Œå°†ä¸ä½¿ç”¨ URL Context")
    
    if tools_list:
        tools = tools_list

    # è¯» Excel
    try:
        wb = load_workbook(xlsx_path)
    except Exception as e:
        print(f"æ— æ³•æ‰“å¼€ Excelï¼š{e}", file=sys.stderr)
        sys.exit(2)

    ws_prompt = get_sheet(wb, "prompt")
    ws_qa = get_sheet(wb, "QA")

    # è¯»å– prompt è¡¨ï¼ˆç¬¬1è¡Œè¡¨å¤´ï¼Œç¬¬2è¡Œå†…å®¹ï¼‰
    prompt_headers = find_header_indexes(ws_prompt)
    if "system" not in prompt_headers or "user" not in prompt_headers:
        print("prompt è¡¨ç¼ºå°‘å¿…éœ€åˆ—ï¼šsystem æˆ– user", file=sys.stderr)
        sys.exit(2)
    
    col_system = prompt_headers["system"]
    col_user = prompt_headers["user"]
    
    sys_prompt = ws_prompt.cell(row=2, column=col_system).value
    user_template = ws_prompt.cell(row=2, column=col_user).value
    
    if sys_prompt is None or str(sys_prompt).strip() == "":
        print("prompt è¡¨çš„ system åˆ—ï¼ˆç¬¬2è¡Œï¼‰ä¸èƒ½ä¸ºç©º", file=sys.stderr)
        sys.exit(2)
    if user_template is None or str(user_template).strip() == "":
        print("prompt è¡¨çš„ user åˆ—ï¼ˆç¬¬2è¡Œï¼‰ä¸èƒ½ä¸ºç©º", file=sys.stderr)
        sys.exit(2)
    
    sys_prompt = str(sys_prompt)
    user_template = str(user_template)
    
    # ä»ç”¨æˆ·æ¨¡æ¿ä¸­æå–è¾“å…¥å­—æ®µ
    input_fields = extract_template_vars(user_template)
    if not input_fields:
        print("user æ¨¡æ¿ä¸­æœªæ‰¾åˆ°ä»»ä½• {{å˜é‡å}} å ä½ç¬¦", file=sys.stderr)
        sys.exit(2)
    
    log(f"ä» user æ¨¡æ¿ä¸­æå–åˆ° {len(input_fields)} ä¸ªè¾“å…¥å­—æ®µ: {input_fields}")

    # QA è¡¨å¤´
    headers = find_header_indexes(ws_qa)
    
    # éªŒè¯è¾“å…¥å­—æ®µæ˜¯å¦éƒ½åœ¨ QA è¡¨ä¸­
    missing_fields = [f for f in input_fields if f not in headers]
    if missing_fields:
        print(f"QA è¡¨ç¼ºå°‘æ¨¡æ¿æ‰€éœ€çš„è¾“å…¥å­—æ®µ: {missing_fields}", file=sys.stderr)
        sys.exit(2)
    
    # ç¡®ä¿æ§åˆ¶åˆ—å­˜åœ¨
    headers = ensure_columns(ws_qa, headers, [COL_FOUND, COL_ERROR])
    col_found = headers[COL_FOUND]
    col_err = headers[COL_ERROR]
    
    # è¾“å…¥å­—æ®µåˆ—æ˜ å°„
    input_cols = {field: headers[field] for field in input_fields}
    
    # è¾“å‡ºå­—æ®µé›†åˆï¼šè¡¨å¤´ä¸­é™¤å»è¾“å…¥å­—æ®µã€FOUNDã€ERROR çš„å…¶å®ƒåˆ—ï¼ˆä»…å†™è¿™äº›ï¼‰
    excluded = set(input_fields) | {COL_FOUND, COL_ERROR}
    output_cols = {k: v for k, v in headers.items() if k not in excluded}

    data_start_row = 2
    data_end_row_initial = ws_qa.max_row  # å¯åŠ¨æ—¶çš„åŸå§‹æœ«è¡Œï¼ˆç”¨äº rows èŒƒå›´ï¼‰
    target_rows = parse_rows_arg(args.rows, data_start_row, data_end_row_initial)

    # ç»Ÿè®¡å»é‡ï¼šåŸºäºè¾“å…¥å­—æ®µç»„åˆå…ƒç»„
    input_tuples_all = []
    input_tuples_done_set = set()
    for r in target_rows:
        # è¯»å–æ‰€æœ‰è¾“å…¥å­—æ®µå€¼
        values = {}
        skip_row = False
        for field, col in input_cols.items():
            val = ws_qa.cell(row=r, column=col).value
            if is_empty_value(val):
                skip_row = True
                break
            values[field] = str(val)
        
        if skip_row:
            continue
        
        # åˆ›å»ºå…ƒç»„ä½œä¸ºå”¯ä¸€æ ‡è¯†
        tuple_key = tuple(values[field] for field in input_fields)
        input_tuples_all.append(tuple_key)
        
        # æ£€æŸ¥æ˜¯å¦å·²å®Œæˆ
        found_v = ws_qa.cell(row=r, column=col_found).value
        if not is_empty_value(found_v):
            input_tuples_done_set.add(tuple_key)
    
    unique_inputs = set(input_tuples_all)
    log(f"è¾“å…¥ç»„åˆå»é‡ç»Ÿè®¡ï¼šæ€» {len(unique_inputs)} ç»„ï¼Œå…¶ä¸­å·²å®Œæˆ {len(input_tuples_done_set)} ç»„")

    # ä¸º rows èŒƒå›´æ‰§è¡Œæ’å…¥åç§»è·Ÿè¸ªï¼šè®°å½•"åŸå§‹ä¸»è¡Œ" -> æ’å…¥çš„é¢å¤–è¡Œæ•°
    inserted_below: Dict[int, int] = {}

    made_backup = [False]

    # ç®€å•çš„è¿›åº¦ç´¯è®¡
    total = len(target_rows)
    n_done = 0
    n_success = 0  # æœ‰ç»“æœ
    n_empty = 0    # æ•°ç»„ç©º
    n_error = 0

    # è´¹ç”¨ç´¯è®¡ï¼ˆå½“ usage å­˜åœ¨æ—¶ï¼‰
    sum_prompt_tokens = 0
    sum_completion_tokens = 0

    retry_call = with_retry(
        lambda *a, **kw: call_llm_genai(*a, **kw),
        retry_times=retry_times,
        retry_delay=retry_delay,
    )

    def current_row_pos(original_row: int) -> int:
        """æ ¹æ®å·²æ’å…¥æƒ…å†µï¼Œè®¡ç®—è¯¥åŸå§‹è¡Œç°åœ¨çš„å®é™…è¡Œå·"""
        shift = 0
        for r0, added in inserted_below.items():
            if r0 < original_row:
                shift += added
        return original_row + shift

    for idx, r0 in enumerate(target_rows, start=1):
        r = current_row_pos(r0)
        
        # è¯»å–æ‰€æœ‰è¾“å…¥å­—æ®µå€¼
        input_values = {}
        empty_fields = []
        for field, col in input_cols.items():
            val = ws_qa.cell(row=r, column=col).value
            if is_empty_value(val):
                empty_fields.append(field)
            else:
                input_values[field] = str(val)
        
        # ç”Ÿæˆå­—æ®µé¢„è§ˆå­—ç¬¦ä¸²ï¼ˆç”¨äºæ—¥å¿—ï¼‰
        preview_parts = [f"{field}='{compact_preview(input_values.get(field, ''), 20)}'" 
                        for field in input_fields]
        input_preview = "[" + ", ".join(preview_parts) + "]"
        
        # åˆ¤å®šæ˜¯å¦è·³è¿‡ï¼ˆæ–­ç‚¹ç»­è·‘ï¼šFOUND éç©ºå°±è·³è¿‡ï¼‰
        found_val = ws_qa.cell(row=r, column=col_found).value
        if not is_empty_value(found_val):
            n_done += 1
            log(f"{idx}/{total} è·³è¿‡ï¼ˆå·²å®Œæˆï¼‰ r={r} {input_preview}")
            continue

        # éªŒè¯è¾“å…¥å­—æ®µä¸èƒ½ä¸ºç©º
        if empty_fields:
            error_msg = f"è¾“å…¥å­—æ®µä¸ºç©º: {', '.join(empty_fields)}"
            ws_qa.cell(row=r, column=col_found, value="é”™è¯¯")
            ws_qa.cell(row=r, column=col_err, value=error_msg)
            save_with_backup_atomic(wb, xlsx_path, made_backup)
            n_done += 1
            n_error += 1
            log(f"{idx}/{total} é”™è¯¯ r={r} {input_preview} -> {error_msg}")
            continue

        # å¡«å……ç”¨æˆ·æ¨¡æ¿
        user_content = fill_template(user_template, input_values)
        
        # è¯·æ±‚
        arr, usage, err = retry_call(
            client, model_id, sys_prompt, user_content, timeout, tools,
            temperature, thinking_budget, args.debug
        )

        if usage:
            sum_prompt_tokens += int(usage.get("prompt_tokens", 0))
            sum_completion_tokens += int(usage.get("completion_tokens", 0))

        if err is not None:
            # å†™å…¥ä¸»è¡Œé”™è¯¯
            ws_qa.cell(row=r, column=col_found, value="é”™è¯¯")
            ws_qa.cell(row=r, column=col_err, value=str(err)[:500])
            save_with_backup_atomic(wb, xlsx_path, made_backup)
            n_done += 1
            n_error += 1
            log(f"{idx}/{total} é”™è¯¯ r={r} {input_preview} -> {err}")
            continue

        # arr ä¸€å®šæ˜¯ list[dict]
        if len(arr) == 0:
            # æ— ç»“æœï¼šä¸»è¡Œå†™"å¦"ï¼Œä¸æ’å…¥æ–°è¡Œ
            ws_qa.cell(row=r, column=col_found, value="å¦")
            ws_qa.cell(row=r, column=col_err, value="")
            # æ¸…ç©ºè¾“å‡ºåˆ—
            for name, c in output_cols.items():
                ws_qa.cell(row=r, column=c, value="")
            save_with_backup_atomic(wb, xlsx_path, made_backup)
            inserted_below[r0] = 0
            n_done += 1
            n_empty += 1
            log(f"{idx}/{total} ç©ºç»“æœ r={r} {input_preview}ï¼ˆå·²è½ç›˜ï¼‰")
            continue

        # æœ‰ç»“æœï¼šä¸»è¡Œå†™ç¬¬1ä¸ªï¼Œä¸‹é¢æ’å…¥ len(arr)-1 è¡Œå†™å…¶ä½™
        # å…³é”®ï¼šåœ¨ä¿®æ”¹ä¸»è¡Œä¹‹å‰ï¼Œå…ˆè¯»å–åŸè¡Œçš„æ‰€æœ‰åˆ—å€¼ï¼ˆç”¨äºæ‹·è´åˆ°å±•å¼€è¡Œï¼‰
        extra = max(0, len(arr) - 1)
        row_values = {}
        if extra > 0:
            # å…ˆä¿å­˜ä¸»è¡Œçš„æ‰€æœ‰åŸå§‹åˆ—å€¼
            for col_idx in range(1, ws_qa.max_column + 1):
                cell_value = ws_qa.cell(row=r, column=col_idx).value
                row_values[col_idx] = cell_value
        
        # ç°åœ¨ä¿®æ”¹ä¸»è¡Œï¼šåªæ›´æ–°è¾“å‡ºå­—æ®µå’Œæ§åˆ¶å­—æ®µï¼Œä¿æŒå…¶ä»–å­—æ®µä¸å˜
        ws_qa.cell(row=r, column=col_found, value="æ˜¯")
        ws_qa.cell(row=r, column=col_err, value="")
        # å†™è¾“å‡ºå­—æ®µï¼ˆåªå†™å…¥ JSON ä¸­å­˜åœ¨çš„å­—æ®µï¼Œé¿å…è¦†ç›–åŸæœ‰æ•°æ®ï¼‰
        first_obj = arr[0]
        for name, c in output_cols.items():
            if name in first_obj:  # åªæœ‰ JSON ä¸­å­˜åœ¨è¯¥å­—æ®µæ—¶æ‰å†™å…¥
                v = first_obj[name]
                if isinstance(v, (dict, list)):
                    v = json.dumps(v, ensure_ascii=False)
                ws_qa.cell(row=r, column=c, value=v)

        # æ’å…¥å±•å¼€è¡Œ
        if extra > 0:
            ws_qa.insert_rows(r + 1, amount=extra)
            # é€æ¡å†™å…¥
            for i in range(extra):
                rr = r + 1 + i
                # å…ˆæ‹·è´ä¸»è¡Œçš„æ‰€æœ‰åˆ—å€¼
                for col_idx, value in row_values.items():
                    ws_qa.cell(row=rr, column=col_idx, value=value)
                
                # ç„¶åè¦†ç›–æ§åˆ¶å­—æ®µ
                ws_qa.cell(row=rr, column=col_found, value="æ˜¯")
                ws_qa.cell(row=rr, column=col_err, value="")
                
                # æœ€åè¦†ç›–è¾“å‡ºå­—æ®µï¼ˆå†™å…¥æ–°ç»“æœï¼Œåªå†™å…¥ JSON ä¸­å­˜åœ¨çš„å­—æ®µï¼‰
                obj = arr[1 + i]
                for name, c in output_cols.items():
                    if name in obj:  # åªæœ‰ JSON ä¸­å­˜åœ¨è¯¥å­—æ®µæ—¶æ‰å†™å…¥
                        v = obj[name]
                        if isinstance(v, (dict, list)):
                            v = json.dumps(v, ensure_ascii=False)
                        ws_qa.cell(row=rr, column=c, value=v)

        inserted_below[r0] = extra
        save_with_backup_atomic(wb, xlsx_path, made_backup)
        n_done += 1
        n_success += 1
        log(f"{idx}/{total} æˆåŠŸ r={r} {input_preview} å±•å¼€ {len(arr)} è¡Œï¼ˆå·²è½ç›˜ï¼‰")

    # ç»“æŸç»Ÿè®¡
    cost = 0.0
    if price_in or price_out:
        cost = (sum_prompt_tokens / 1_000_000.0) * price_in + (sum_completion_tokens / 1_000_000.0) * price_out

    log("å®Œæˆã€‚")
    log(f"- æ€»è®¡ï¼š{total}, æˆåŠŸ(æœ‰ç»“æœ)={n_success}, ç©ºç»“æœ={n_empty}, é”™è¯¯={n_error}")
    if (sum_prompt_tokens + sum_completion_tokens) > 0:
        log(f"- tokens: prompt={sum_prompt_tokens}, completion={sum_completion_tokens}, ä¼°ç®—è´¹ç”¨=${cost:.4f}ï¼ˆæŒ‰é…ç½®å•ä»·ï¼‰")


if __name__ == "__main__":
    main()

